{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import imgaug.augmenters as iaa\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers, models\n",
    "from keras import applications\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Random Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_augmenter = iaa.Sequential([\n",
    "    # Horizontal Flip\n",
    "    iaa.Fliplr(0.5),\n",
    "\n",
    "    # Vertical Flip\n",
    "    iaa.Flipud(0.5),\n",
    "\n",
    "    # Multiply\n",
    "    iaa.Multiply((0.8, 1.2)),\n",
    "\n",
    "    # Linear Contrast\n",
    "    iaa.LinearContrast((0.6, 1.4)),\n",
    "\n",
    "    # Affine\n",
    "    iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "               rotate=(-30,30),\n",
    "               scale=(0.5, 1.5)),\n",
    "\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Images and Labels into two different arrays with Augmentation and Pre-processing (imgaug) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Dataset/'\n",
    "dataset_path = os.listdir('Dataset')\n",
    "image_size = 224\n",
    "class_labels = []\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for label in dataset_path:\n",
    "    data_path = path + str(label)\n",
    "    filenames = [i for i in os.listdir(data_path)]\n",
    "\n",
    "    for filename in filenames:\n",
    "        img = cv2.imread(data_path + '/' + filename)\n",
    "        img = cv2.resize(img, (image_size, image_size))\n",
    "        \n",
    "        # image augmentation x10 (random augmentations)\n",
    "        for i in range(10):\n",
    "            class_labels.append((label, str('dataset_path' + '/' + label) + '/' + filename))\n",
    "            augmented_image = random_augmenter(image=img)\n",
    "            images.append(augmented_image)\n",
    "            labels.append(label)\n",
    "            # cv2.imshow(\"Augmented Image\", augmented_image)\n",
    "        # cv2.waitKey(0)\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame(data=class_labels, columns=['Labels', 'Image'])\n",
    "\n",
    "images = np.array(images)\n",
    "images = images.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the Labels + Splitting the dataframe to train-test sets (80-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cassava__Unidentified_1' 'Cassava__Unidentified_1'\n",
      " 'Cassava__Unidentified_1' ... 'Sweet_Potato__Unidentified_1'\n",
      " 'Sweet_Potato__Unidentified_1' 'Sweet_Potato__Unidentified_1']\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "df_labels = dataframe['Labels'].values\n",
    "print(df_labels)\n",
    "\n",
    "df_labelEncoder = LabelEncoder()\n",
    "df_labels = df_labelEncoder.fit_transform(df_labels)\n",
    "print(df_labels)\n",
    "\n",
    "df_labels = df_labels.reshape(-1, 1)\n",
    "ct = ColumnTransformer([('my_ohe', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "Y = ct.fit_transform(df_labels)\n",
    "\n",
    "images, Y = shuffle(images, Y, random_state=0)\n",
    "train_x, test_x, train_y, test_y = train_test_split(images, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the EfficientNetB0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(dataset_path)\n",
    "IMG_SIZE = image_size\n",
    "size = (IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "outputs = applications.EfficientNetB0(include_top=True, weights=None, classes=NUM_CLASSES)(inputs)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model by 30 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_x, train_y, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'Model_EfficientNetB0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
